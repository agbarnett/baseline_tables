---
title: "**Automated detection of potential problems in baseline data in randomised controlled trials**"
author:
  - Adrian Barnett ^[Australian Centre for Health Services Innovation & Centre for Healthcare Transformation, Queensland University of Technology, Queensland, Australia, a.barnett@qut.edu.au]
  - John Carlisle   ^[NHS, UK]

output:
  bookdown::word_document2:
    number_sections: false
    reference_docx: bmj-open-style.docx
    citation_package: natbib
bibliography: references.bib
csl: bmj-open.csl
---

Word count: ?

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
source('../99_functions.R')
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(gridExtra)
library(officer) # for border
library(flextable)
std_border  = fp_border(color="black") # for flextable
```


\newpage

# Abstract

*Objectives*: 

*Design*: Observational study. 

*Setting*: Published randomised controlled trials in open access journals. 

*Participants*: 

*Main outcome measures*: 

*Results*: 

*Conclusion*: 

*Key words*: randomised controlled trials; research misconduct


\newpage

# Introduction

Papers describing the results of a randomised trial often include a table that compares the randomised groups at baseline (hereafter called a "baseline table"). 
This baseline table presents summary statistics that describe the sample such as age and gender.
The table's purpose is to demonstrate that the randomisation produced similar groups, which strengthens the case that any differences between groups are due to the randomised treatment.
This table is recommended by the CONSORT checklist which was designed to improve the reporting of randomised trials [@Schulz2010]. 

If randomisation has been successful, then the summary statistics will be similar across all groups at baseline, although large differences can happen simply due to chance, especially for small sample sizes. 
To help demonstrate similarity, many authors compare the randomised groups using statistical tests, despite the circularity of testing if randomisation happened by chance [@Altman1985]. 
<!--- Even in ideal circumstances, 1 in 20 of these tests would have a p-value below the 0.05 threshold.  --->

Researchers who fabricated randomised trials have been discovered because their baseline tables were too perfect [@Adam2019;@Carlisle2020]. When fabricating the baseline table they created highly comparable groups that would pass peer review. In not trying to raise any alarms during peer review, they unwittingly raised the alarm at post peer review. Fraudulent researchers can also create baseline data with unusually large differences between groups, likely because they do not understand how to create realistic summary statistics when data are truly random. Fraudulent researchers may not be uncovered by one baseline table alone, but an odd table can be the start of a wider investigation of their work [@Li2020]. 

To date, fraudulent researchers have been found by researchers noticing strange patterns whilst reading papers in their field, sometimes by systematic reviews and meta-analysis. Other problems have been found by dedicated researchers trawling through papers by hand. Manually extracting data from papers is time consuming and automatic data extraction would be a useful advance. Automated detection tools would save time and increase the current level of scrutiny [@Bordewijk2021].

<!--- "we advise automating these methods as much as possible. Automation of “ready” methods would promote wide use" --->

Previous methods for spotting potential problems in baseline tables have re-calculated p-values for the tests comparing groups at baseline, and then tested if the distribution of p-values was uniform [@Carlisle2015]. However, it is possible to get a non-uniform distribution of p-values when the two groups were randomised, e.g., for skewed data [@Bland2013]. Another limitation with this approach is that it has only been applied to summary statistics of the mean and standard deviation for continuous variables, which means that percentages are lost. This is a large loss as percentages are commonly used in baseline tables. In this paper we avoid the issues of using the distribution of p-values by instead examining the distribution of t-statistics. We test whether the distribution of t-statistics is too narrow or too wide compared with a typical randomised trial. 

The aim of this paper is not to provide undeniable evidence for fraud. Baseline tables that appear to have a problem could occur for a range of non-fraudulent reasons. These include planned or unplanned factors to do with randomisation, such as dynamic randomisation to create highly comparable groups, or subversion of the random allocation [@Kennedy2017]. Differences could also arise at the reporting stage, such as mislabelling summary statistics or simple cut-and-paste errors. Our aim is to create a procedure that could be used to flag potential problems at the submission stage, which would help researchers improve their paper prior to publication [@Weissgerber2021].

# Methods

## Automated extraction of baseline tables

We extracted baseline tables from the National Library of Medicine's PubMed Central open access subset which has 3.7&nbsp;million papers. The basic steps were:

* Download a list of randomised trials from the _trialstreamer_ web page [@Marshall2020] using the Pubmed Central ID (PMCID). The _trialstreamer_ data was downloaded on 9&nbsp;August 2021 and had 57,109 trials with a PMCID. 
* Access the available papers from the open access subset using the PMCID; download the paper in XML format and read it into R.
* Exclude papers that were not randomised trials, including i) protocols which have no data, and ii) papers that re-use trial data for other study designs (e.g., diagnostic accuracy). This exclusion was made based on the title and abstract, but some protocols were not identified in the title or abstract and hence were still included.
* Find the baseline table in the paper and extract all the summary statistics, the type of summary statistics (e.g., median or percentage), and the group sample sizes.

A key step was estimating what summary statistic were used in each row of the baseline table. This was estimated based on the text in the rows and columns (e.g., "Mean", "%", etc) and the variable label, where particular variables such as age and BMI were often continuous, whereas variables such as gender were categorical.

Another key step was estimating the group sample size. These were first estimated by checking for headings in the column headers (e.g., "N=") and then the rows. If no samples sizes could be found then they were estimated based on percentages in the baseline table. Some papers were excluded because no sample size could be extracted (see results).

## Validation

We validated our algorithm using hand-entered baseline data from randomised trials. To find eligible trials we searched _PubMed_ for randomised trials between 2017 and 2020 that were available open access on PubMed Central and were not protocols, which gave 25,760 trials. A random selection of 200 trials was made, with the results compared between the algorithm and manually entered data by a post-doctoral research assistant. 118 papers were excluded by the algorithm, with the three most common reasons because the paper was not openly available (48), there was no baseline tables (36), or there was no comparison between groups in the table (16). A further 9 papers could not be compared because the research assistant judged them not to be randomised trials. This left 73 baseline tables to compare. 

Detailed comparisons are given in a supplement. In summary, our algorithm correctly guessed the summary statistic 87% of the time (795 out of 909; 95% CI 85% to 90%). The biggest differences were where the algorithm wrongly guessed continuous for a median (2%) or could not estimate any statistic when the row was a percentage (5%). There was a mean difference in the sample size of 0 (5% to 95% percentile: 0 to 0). The algorithm accurately estimated the size of the baseline table, with a median difference of 0 (5% to 95% percentile: --1 to 4) for the number of rows, and 0 (5% to 95% percentile: 0 to 0) for the number of randomised groups. 

We think this level of accuracy is reasonable as we encountered a large variety in the presentation of baseline tables, an issue that has been flagged by others [@Cahan2017]. Failures of the algorithm sometimes meant the table data were excluded as our approach struggled to extract the numbers, this means that we are not able to completely screen the literature. Failures where the wrong data were extracted (e.g., total columns instead of group columns) sometimes led to the trial being flagged by our model and we examine this issue in the results. 

We compared the total sample size in our algorithm with that estimated by the _trialstreamer_ program [@Marshall2020] for the 1,684 trials which had estimated sample sizes from both approaches. For 71% of trials the difference was within $\pm$ 10%. Some of the larger differences occurred because _trialstreamer_ estimated the sample size based on the abstract, whereas our algorithm used the baseline table and there were trials that only used a subset of the full data (e.g., PMC6746080). There were also occasions where _trialstreamer_ extracted a year rather than the number randomised (e.g., PMC4852093), and where _trialstreamer_ used the number of clusters in a cluster randomised trial whereas our algorithm used the number of participants (e.g., PMC7690135).

## Creating t-statistics for group differences

We combined continuous and binary summary statistics by summarising the difference between randomised groups using the independent sample t-statistic, which put the group differences on a common scale. It may be surprising to use the t-statistic to compare categorical data, but the t-test is robust in situations where the chi-squared test would be a common choice, even for small sample sizes [@Dagostino1988]. Combining binary and continuous summary statistics greatly increases the amount of data we examined as these two statistics were the most commonly used; previous approaches have only examined continuous data [ref]. We could not use statistics that were the median and inter-quartile range or range, as we could not compare these summary statistics using the parametric t-test. 

We used all available table rows, which for some tables will include correlated data, for example tables that included rows for the percentage of males and females. In this case the t-statistics for males and females would be perfectly negatively correlated. We ignored this correlation in our analysis as most tables had many rows and hence the effect any correlation should be diluted. Also, whilst correlated rows for gender would be easy to identify, identifying other correlations would be more challenging. In a sensitivity analysis we examined the effect of removing correlated rows for gender.  

We created t-statistics for all pairs of comparisons. For example, a trial with three treatment arms would have three comparisons (A vs B, A vs C, B vs C). This potentially created correlation in the t-statistics, which we did not adjust for as the approach screens for overall in the baseline table, and in cases of fraud the problems may well appear for all groups. 

## Bayesian model of observed differences

The observed differences ($d$) in randomised groups were modelled using a t-distribution.
$$d_{i,j} \sim t(0, \sigma^2_{i,j}, \textrm{df}_{i}), \qquad i=1,\ldots,M,\, j=1,\ldots, n_i,$$
where $i$ is the study index and $j$ is the row in the table. The difference is either the difference in reported means for continuous statistics, or the difference in reported proportions for categorical statistics. The mean difference is zero which is as expected for randomised groups at baseline. 

The pooled variance ($\sigma^2_{i,j}$) either uses the reported standard deviations for continuous statistics or reported proportions for categorical. The pooled inverse-variance was modelled as 
$$\sigma^{-2}_{i,j} = s_{i,j}^{-2} \times \gamma_i,$$
where $s$ is the reported pooled variance. The study-level random variable $\gamma_i$ was used to model a difference in the precision for study $i$ using a spike-and-slab approach [@Ishwaran2005]:
$$\log(\gamma_i) = [(1-p_i)\times 0] + [p_i \times \epsilon_i],$$
$$p_i \sim \textrm{Bernoulli}(\theta),$$
$$\theta \sim \textrm{Beta}(1,1),$$
$$\epsilon_i \sim \textrm{N}(0,10).$$
Each study has a "switch" $p_i \in (0,1)$, that determines whether it is part of the spike or slab. 
The spike at zero, with $p_i =0$, is for studies where the differences between randomised groups are as expected ($\sigma^{-2}_{i,j} = s_{i,j}^{-2}$). The slab, with $p_i =1$, is for studies where the variance is smaller or larger than expected. This difference was modelled using a normal distribution where a negative $\epsilon_i$ indicates results are too variable ($\gamma_i<1$) and a positive $\epsilon_i$ indicates results are too precise ($\gamma_i>1$). The variance for this normal prior of 10 may seem small compared with typical vague priors in Bayesian models, but this covered the full range of possibilities, including where the statistics were identical between randomised groups, and larger variances had convergence issues. The binary switch for each study is modelled using a Bernoulli distribution. The probability $\theta$ is the expected proportion of studies with a potential problem, which had a vague Beta prior. 

The degrees of freedom (df) for study $i$ is the total sample size minus one ($N_i-1$), which allows for greater variance in differences for smaller studies. 

Two statistics can be used to judge whether a baseline table has a potential problem:

* The estimated study-specific probability of a non-zero additional precision $\overline{p}_i = \sum_{j=1}^M p_{i}^{(j)} / M$, for which we set a threshold of $\overline{p}_i >0.8$ to flag a potential problem by averaging over the $M$ Markov Chain Monte Carlo estimates
* The estimated precision $\overline{\epsilon}_i$ which indicates larger problems for values further from zero.

We report whether a baseline table has a potential problem, but we do not attempt to differentiate between fraud and honest errors [@George2015]. We report PubMed Central ID numbers to highlight examples without citing papers, and make no imputation about whether an issue is fraud or error. Checking for fraud needs to be done by checking other details, such as ethics clearances, and referencing other work by the same authors. However, our automated check could still be a useful flag when papers are submitted.

Our approach uses the baseline table reported by the authors and hence is reliant on reporting accuracy.
Inaccurate reporting could lead to issues being flagged incorrectly, for example, authors reporting the standard error of the mean but incorrectly labelling it as the standard deviation.
We show below how our model frequently flagged studies where our automated data extraction of the baseline table was unsuccessful _or where reporting errors..._. 

## Graphical description of the model

A simple graphical description of the model is shown in Figure&nbsp;\@ref(fig:fig-graphical), which shows hypothetical results for two studies each with 40 comparisons in their baseline table [@Carlisle2015]. 

```{r fig-graphical, fig.cap='Hypothetical distributions of the t-statistics from two studies each with 40 rows in their baseline table. The green line shows a normal distribution fitted to the data.', fig.width=7, fig.height=4}
# plot random data plus normal curve, see https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve
s_size = 40
TeachingDemos::char2seed('exeter') # to give consistent results
study1 = data.frame(value=rnorm(n=s_size, mean = 0, sd=1), group=1)
study2 = data.frame(value=rnorm(n=s_size, mean = 0, sd=0.2), group=2)
bw = 0.2; # bin width
to_plot = bind_rows(study1, study2) %>%
  mutate(facet = factor(group, levels=1:2, labels=c("Precision as expected","Narrow precision")))
curves = group_by(to_plot, group) %>% 
  nest(data = value) %>% 
  mutate(y = map(data, ~ dnorm(
    .$value, mean = mean(.$value), sd = sd(.$value)
    ) * bw * sum(!is.na(.$value)))) %>% 
  unnest(c(data, y)) 
# plot
gplot = ggplot(curves, aes(x = value)) +
  geom_histogram(data = to_plot, binwidth = bw, fill = "khaki3") +
  geom_line(aes(y = y), col='darkseagreen3', size=1.05) + 
  theme_bw()+
  scale_x_continuous(limits=c(-3, 3))+ # make symmetric
  facet_wrap(~ facet)+
  ylab('Number of results')+
  xlab('Observed t-statistics')
gplot
```

Both example studies have a mean difference of zero, but the observed differences from one study are much narrower and closer to zero, indicating that the randomised groups are too similar. Our approach models the precision (inverse-variance) and aims to detect when studies have an unexpectedly narrow or wide precision for the distribution of all available t-statistics.

## Simulation study

To test the benefit of using the t-distribution instead of the distribution of p-values, we simulated data using three examples described by Bland [@Bland2013] where there was no concern about randomisation but the p-value distribution would be non-uniform:

* Small studies with a sample size of 10 and comparisons using binary data (e.g., percent of males)
* Large studies with a sample size of 1,000 and comparisons using binary data
* Large studies with a sample size of 1,000 and comparisons using skewed continuous data (e.g., length of hospital stay)

To show the power of the method to detect problematic tables, we used three additional scenarios that used a mix of binary and continuous data where the summary statistics for the baseline table were: 

* As expected for a randomised trial
* Too precise, randomised groups are too similar
* Too variable, randomised groups are too different

The group sample sizes for these three scenarios were randomly generated using an exponentiated gamma distribution with shape of 10.7 and rate of 2.84, which gives a median sample size of 40 and inter-quartile range of 19 to 87. The number of rows per baseline table were randomly generated study using a gamma distribution with shape 2.2 and rate 0.15, which gives a median number of rows of 12 and inter-quartile range of 7 to 19. The parameters for the group sample size and table row distributions were based on a large sample of the baseline table data from our automated extraction.

For all six scenarios we created 500 simulated trials and each trial had two groups with equal sample size. We compared the statistics ...

## Estimation

All models were estimated using a Bayesian approach using the WinBUGS software Version&nbsp;1.4.3 [@Lunn2000]. The data management and plotting were made using R version&nbsp;4.1.1 [@R].
All the R code to extract the tables is openly available https://github.com/agbarnett/baseline_tables.

## Ethical considerations

We used publicly available data that are published to be read and scrutinised by researchers and hence ethical approval was not required.

# Results

## Simulation results

```{r, results='asis'}
load('../results/uniform_bland.RData') # from 5_run_summary.R
# tidy results slightly
edited_table = arrange(overall_table, h0, sgroup) %>%
  mutate(
    `No issue` = roundz(`No issue`*100/500, 1),
    `Too precise` = roundz(`Too precise`*100/500, 1),
    `Too variable` = roundz(`Too variable`*100/500, 1),
    algorithm = paste(`No issue`, " / ", `Too precise`, " / ", `Too variable`, sep='')) %>%
  select(h0, sgroup, algorithm, prop) %>%
  rename('Difference in groups' = 'h0',
         'Not flagged / Too precice / Too variable (%)' = 'algorithm',
         'Simulation type' = 'sgroup',
         'Uniform test rejected (%)' = 'prop')
# table
ftab = flextable(edited_table) %>%
  merge_v(j=1) %>%
  fontsize(size=10, part = 'all') %>%
  width(j = 1, width = 1) %>%
  width(j = 2, width = 1.2) %>%
  width(j = 3, width = 1.5) %>%
  width(j = 4, width = 1.1) %>%
  theme_box() 
ftab
```

<!--- Using the p-value distribution did not perform as well. --->
The table shows the results for the six simulated scenarios. Four scenarios were generated using the null hypothesis of no difference between groups. However, using the uniform distribution test did badly here, especially when the data were skewed. Examples for single simulations are shown in supplement&nbsp;2 and show the non-uniform distribution when the data are skewed. 

Our algorithm very rarely flagged studies where there was no difference in group, hence there were few false positives. 
Our algorithm was highly successful at detecting trials that were too variable, with 89% of the simulations flagged. 
Our algorithm was less successful at detecting trials that were too precise, with just 13% of the simulations flagged. 

## Results for real data

```{r, include=FALSE}
# get results from Bayesian model
setwd('..') # move to main R folder
load("results/uniform_trialstreamer.RData")
# get data for summary stats
source =  'trialstreamer'
stage = 'plot'
source('1_which_data_source.R') # uses `source` and `stage`
# summary stats
N = nrow(for_model)
N_trials = length(unique(for_model$pmcid)) 
# row and column stats
stage = 'model'
source('1_which_data_source.R') # uses `source` and `stage`
tab_stats = group_by(table_data, pmcid) %>%
  summarise(nr = max(row),
            nc = max(column)) %>%
  ungroup() %>%
  summarise(row = median(nr),
            col = median(nc)) 
setwd('paper') # move back to paper folder
```


There were `r N_trials` included trials with a total of `r format(N,big.mark=',')` table rows. The median number of rows per trial was `r tab_stats$row` and the median number of columns per trial was `r tab_stats$col`.

```{r tab-real, tab.cap='Results for the real data. Number and percentages of trials that were not flagged or were flagged as too precise or too variable.'}
ftab = select(overall_table, group, n.x, percent) %>%
  rename('n'= 'n.x',
         'Algorithm category' = 'group') %>%
  flextable() %>%
  theme_box() %>%
  autofit()
ftab
```

The results for the real data are in Table&nbsp;\@ref(tab:tab-real) and show that most trials (71.2%) had no issue. The most common issue was results that were too variable (23.1%), with fewer trials being too precise (5.8%).

The t-distributions for three studies that were flagged as too variable are plotted in Figure&nbsp;\@ref(fig:fig-examples). For comparison, three randomly selected studies with no issue are also plotted. The three flagged studies were selected using $\epsilon$ and hence show the most extreme results.

```{r fig-examples, fig.cap='Three example t-distributions flagged as too variable. For comparison there are three randomly selected trials that were not flagged. The scales on the y-axis differ by trial.', fig.width=7, fig.height=4}
```

The figure shows the potentially large difference between a study where the t-distribution is too wide compared with another .

Some example results are ...

# Discussion

In our method, one unusual result can mean that the t-distribution is flagged as too variable, whereas the method that examines the uniformity of the p-value would likely be hidden.

At times our algorithm flagged papers where the baseline table was not a baseline table for a randomised trial, but was a study that re-used the data from a trial. For example, a study (PMC7660513) which examined responders and non-responders to a randomised treatment, and the table that we extracted which included demographics compared non-randomised groups and hence the expected differences are not zero. It is challenging to exclude these studies as the abstract and title naturally talk about the randomised trial. Any automated flags raised for papers like this would need to be filtered by a person, or the authors whose study was flagged could quickly explain that it was not the results of a randomised trial.

What percentage of trials would we expect to be flagged? Our results were that 29% were flagged as having some issue. A study of randomised trials submitted to the journal _Anaesthesia_, estimated that around one quarter of trials had false data that was problematic enough to invalidate the trial [@Carlisle2020], which is similar to our estimate.

Better to prevent the publication of false data in the first place [@Carlisle2020].

Easier to detect problems in individual patient data rather than summary statistics [@Carlisle2020]. The methods could be extended to study individual patient data, and journals could request the data at submission. "Individual patient data increased the detection of false trial data about 20-fold compared with trials without spreadsheets." [@Carlisle2020]

Decimal places ... An estimate of inappropriate rounding down is x% from abstracts [barnett, f1000].

## Relation to previous work

A number of other automated tools have been used to detect problems with papers, including statcheck for statistical tests and sprite for x. , 

Previous work has tested for a uniform distribution in the p-values from baseline tables. However, there are circumstances in which the p-values will not be uniform [@Bland2013]. Difficulties reconstructing p-values from the original data. The methods presented here convert both categorical and continuous variables to a standardised scale and so easily combine the two most commonly used summary statistics. This gives increased power to detect problematic papers. 

We looked for issues using the t-statistic, but other detections could be applied to the baseline table data such as Benford's law [@George2015; @Bradshaw2021].

## Limitations

There are a number of potential refinements that would improve the ability of the model to detect issues. Correlated rows could be excluded, for example if the baseline table included rows for the percentages of men and women, then one row could be removed. 

# Conclusion

The integrity of randomised trials is key to evidence-based medicine because of their influence on practice directly and via meta-analysis.

## Acknowledgements

## Competing interests

The authors have declared that no competing interests exist.

## Funding

This work was supported by National Health and Medical Research Council (https://www.nhmrc.gov.au/) grant number APP1117784. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.

## Authors' contributions


## Data sharing

All the code to extract the data and run the analyses are freely available online: https://github.com/agbarnett/baseline_tables.

# References {#references}
