---
title: "Results from the Bayesian model for flagging trials"
author: "Adrian Barnett"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: word_document
---

```{r, include=FALSE}
## called by 5_run_summary.R ###

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
options(width=1000) # Wide pages
options(scipen=999) # avoid scientific presentation
library(ggplot2)
library(dplyr)
library(janitor) # for cross-tabulations
library(flextable)
library(stringr)
library(tidyverse)
library(ggformula) # for density plots
library(uniftest) # for test of uniform distribution
source('99_functions.R')

# get the bugs results
load(infile)
```

```{r}
# get mean and credible interval for every node
all_stats = reshape2::melt(bugs$sims.matrix) %>%
  group_by(Var2) %>%
  summarise(n = n(),
            mean = mean(value),
            median = median(value),
            lower = quantile(value, 0.025),
            upper = quantile(value, 1 - 0.025)) %>%
  ungroup() %>%
  mutate(Var2 = as.character(Var2),
         strip = str_remove_all(Var2, '[^0-9|,]'),
         nicevar = case_when(
    str_detect(Var2, '^mu.var')  ~ 'Precision',
    str_detect(Var2, '^var.flag') ~ 'Precision flag',
    str_detect(Var2, '^p_precision') ~ 'P precision')
    ) %>%
  separate(col=strip, into=c('study','index'), sep=',', convert=TRUE) %>% # split on comma, can ignore warnings
  rename('Variable' = 'Var2') %>%
  left_join(study_numbers, by='study') %>% # add PMCID
  mutate(pmcid = ifelse(nicevar=='P precision', NA, pmcid)) # merge does not work for this variable

# add study type if it is a simulation
if(simulation == FALSE){ # from 1_which_data_source.R
  all_stats = mutate(all_stats, sgroup='None', h0 = 'N/A')
}
if(simulation == TRUE){ # from 1_which_data_source.R
  all_stats = mutate(all_stats,
                     h0 = case_when( # group for plotting
str_detect(pmcid, pattern='small') ~ 'No',
str_detect(pmcid, pattern='large') ~ 'No',
str_detect(pmcid, pattern='skewed') ~ 'No',
str_detect(pmcid, pattern='rounded') ~ 'No',
str_detect(pmcid, pattern='none') ~ 'No',
str_detect(pmcid, pattern='precise') ~ 'Yes',
str_detect(pmcid, pattern='variable') ~ 'Yes'
  ),
      sgroup = case_when( # group for plotting
str_detect(pmcid, pattern='small') ~ 'Small binary',
str_detect(pmcid, pattern='large') ~ 'Large binary',
str_detect(pmcid, pattern='skewed') ~ 'Skewed',
str_detect(pmcid, pattern='rounded') ~ 'Rounded',
str_detect(pmcid, pattern='none') ~ 'As expected',
str_detect(pmcid, pattern='precise') ~ 'Under-dispersed',
str_detect(pmcid, pattern='variable') ~ 'Over-dispersed'
  ))
}

# flags in vector
flag = filter(all_stats, str_detect(string=Variable, 'flag')) 

# save for 6_tree_flag.R
save(all_stats, file=tree_outfile) # change file dependent on data
```

The results are based on `r max(for_model$study)` trials. These results are for the `r source` sample. 

The MCMC estimates were `r bugs$n.iter` iterations thinned by `r bugs$n.thin` from `r bugs$n.chains` chains with a burn-in of `r bugs$n.burnin`.

# Probabilities

The table shows the marginal probabilities for the precision probability. This probability controls the proportion of trials that are flagged as having some issue.

```{r}
probs = filter(all_stats, str_detect(Variable, '^p_')) %>%
  select(nicevar, mean, lower, upper) %>%
  rename('Variable' = 'nicevar')
ftab = flextable(probs) %>%
  theme_box() %>%
  colformat_double(digits=3) %>%
  autofit() 
ftab
```

The table shows the mean and 95% credible interval.

# Flag means

The histogram shows the mean flag for each study. The precision flags are 0 = ideal variance, 1 = over- or under-dispersed.

```{r}
gplot = ggplot(data=flag, aes(x=mean))+
  geom_histogram(fill='grey66')+
  xlab('Mean flag')+
  theme_bw()
if(simulation == TRUE){ # add facet if it is a simulation
  gplot = gplot + facet_wrap(~sgroup)
}
gplot
```

# Precision 

```{r}
# just results that are not in the spike
non_spike = filter(all_stats, 
                   nicevar == 'Precision flag',
                   median !=0) %>%  # 
  select(study)
plot_precision = filter(all_stats, nicevar=='Precision') %>%
  right_join(non_spike, by='study') %>%
  mutate(facet = ifelse(mean < 0, 'Over-dispersed', 'Under-dispersed'))
# truncate at 10^6 to make plot readable
cut = 10^6
trun = nrow(filter(plot_precision, exp(mean) > cut))
plot_precision = filter(plot_precision, exp(mean) <= cut)
# plot
gplot_precision = ggplot(data=plot_precision, aes(x=exp(mean)))+
  geom_histogram(fill='grey66')+
  geom_vline(xintercept = 1, lty=2)+
  theme_bw()+
  scale_x_log10()+
  xlab('Mean precision multiplier (log-10 scale)')+
  ylab('Count')+
  facet_wrap(~facet, scales='free')
gplot_precision
save(cut, trun, gplot_precision, file=precision_outfile)
```

The plot is for trials where the median flag was non-zero. 

We truncated the plot at `r format(cut, big.mark=',')` as a few extreme outliers made it difficult to visualise the distribution. There were `r trun` results truncated.

#### Means and intervals

```{r}
# max numbers to randomly select
max_numbers = group_by(plot_precision, facet) %>%
  tally()
max_select = min(20, min(max_numbers$n)) # if fewer than 20 studies per group, then change 
# max
for_plot = group_by(plot_precision, facet) %>%
  sample_n(size=max_select, replace=FALSE) %>% # randomly select a few results
  mutate(studynum = as.numeric(as.factor(study))) %>% # re-order study number without gaps for plotting (within facet)
  ungroup()
#
gplot = ggplot(data=for_plot, aes(x=studynum, y=exp(mean), ymin=exp(lower), ymax=exp(upper)))+
  geom_point(col='dodgerblue')+
  geom_hline(yintercept=1, lty=2, col='dark red')+
  geom_errorbar(width=0, col='dodgerblue')+
  theme_bw()+
  xlab('')+
  scale_y_log10()+
  ylab('Mean precision (log-10 scale)')+
  coord_flip()+
  facet_wrap(~facet, scales='free')
gplot
```

Forest plot of means and 95% credible intervals. Results for a random selection of 20 trials per group. The dotted line at 1 is no change in precision. 

#### Means and intervals by study size

```{r, fig.width=7}
for_plot = mutate(plot_precision,
                  ci_width = upper - lower)
#
gplot = ggplot(data=for_plot, aes(x=size, y=ci_width))+
  geom_point()+
  geom_hline(yintercept=0, lty=2, col='dark red')+
  theme_bw()+
  xlab('Sample size')+
  scale_x_log10()+
  ylab('Credible interval width')+
  facet_wrap(~facet, scales='free')
if(simulation == TRUE){ # add color if it is a simulation - to do
  gplot = gplot + geom_point(data=for_plot, aes(x=size, y=ci_width, col=sgroup))
}
gplot
```

#### Frequency table of categories

```{r}
# threshold for P
threshold = c(0.5,0.8,0.99) # use three thresholds
# table using three categories
medians = filter(all_stats, 
                nicevar == 'Precision flag') %>%
  select(study, mean) %>%
  rename('p_mean' = 'mean')
means = filter(all_stats, nicevar=='Precision') %>%
  select(study, sgroup, mean) %>%
  right_join(medians, by='study') %>%
  mutate(group1 = 
           case_when(
             p_mean <=  threshold[1] ~ 'No issue',
             p_mean > threshold[1] & mean <= 0 ~ 'Over-dispersed', 
             p_mean > threshold[1] & mean > 0 ~ 'Under-dispersed'),
         group2 = 
           case_when(
             p_mean <=  threshold[2] ~ 'No issue',
             p_mean > threshold[2] & mean <= 0 ~ 'Over-dispersed', 
             p_mean > threshold[2] & mean > 0 ~ 'Under-dispersed'),
         group3 = 
           case_when(
             p_mean <=  threshold[3] ~ 'No issue',
             p_mean > threshold[3] & mean <= 0 ~ 'Over-dispersed', 
             p_mean > threshold[3] & mean > 0 ~ 'Under-dispersed'))
  # separate frequencies for each threshold
  for_plot = pivot_longer(means, cols = starts_with('group')) %>%
    group_by(sgroup, name, value) %>%
    tally() %>%
    group_by(sgroup, name) %>%
    mutate(
      percent = prop.table(n)*100,
      threshold = case_when(
      name == 'group1' ~ threshold[1],
      name == 'group2' ~ threshold[2],
      name == 'group3' ~ threshold[3]
    )) %>%
    ungroup()
  my_tab = select(for_plot, sgroup, threshold, value, percent) %>%
    mutate(percent = roundz(percent, 1)) %>%
    group_by(sgroup, threshold) %>%
    pivot_wider(values_from = percent, names_from = value) %>%
    mutate(across(everything(), ~replace_na(., 0))) %>% # replace NA with 0
    rename('simulation' = 'sgroup') %>%
  ungroup()
if(simulation == TRUE){
ftab_results = flextable(my_tab) %>%
  theme_box() %>%
  merge_v(j=1) %>%
  autofit()
}
if(simulation == FALSE){
ftab_results = select(my_tab, -simulation) %>%
  flextable() %>%
  theme_box() %>%
  autofit()
}
ftab_results
```

We used three thresholds of `r paste(threshold, collapse=' and ', sep='')` for the trial-level probability of an additional precision.

#### Plot of above

The plot below shows the same results as the table.

```{r}
p_plot = ggplot(data=for_plot, aes(x=factor(threshold), y=percent, fill=value))+
  geom_bar(stat='identity', position='stack')+
  scale_fill_manual(NULL, values=c('darkseagreen1','indianred1','orange4'))+
  theme_bw()+
  xlab('Threshold or uniform test')+
#  xlab('Probability threshold for a non-zero change in the precision')+
  ylab('Percent')+
  theme(legend.position = 'right',
        panel.grid.major = element_blank())
if(simulation==TRUE){ # add facet if simulation
  p_plot = p_plot + facet_wrap(~sgroup) +
    theme(legend.position = c(0.8,0.2))

}
p_plot
```

# Plots of t-statistics

Here we show plots of the t-statistics from the trials that were flagged as over- or Under-dispersed.
The top three results were chosen based on the value of $\epsilon$ and hence show the most extreme examples. For comparison we plot three randomly selected trials that were not flagged.

# a) Over-dispersed

```{r, fig.width=10, fig.height=7}
## show the t-stats of selected papers
selected_papers = select_papers(
  in_data = for_model,
  stats = all_stats,
  flag = 'Precision flag',
  flag_value = 1, # results that get flagged (could lower threshold?)
  mean_select = 'negative', # Over-dispersed
  flag_null_value = 0, # results that are not flagged for comparison
  variable = 'Precision',
  n_select = 3
)
gplot_over = ggplot(data=selected_papers, aes(x=t, fill=factor(type)))+
  scale_fill_manual("Potential\nissue", values=c('tomato2','grey66'), labels=c('Yes','No'))+
  geom_histogram()+
  xlab('t-statistic')+
  ylab('Count')+
  scale_y_continuous(labels = label_number(accuracy=1))+ # no decimal places in 
  facet_wrap(~pmcid_ordered, scales='free_y')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())
gplot_over
```

# a2) Over-dispersed after excluding extremely small multipliers

```{r, fig.width=10, fig.height=7}
exclude  = 0.01
## show the t-stats of selected papers
selected_papers = select_papers(
  in_data = for_model,
  stats = all_stats,
  exclude = exclude, 
  flag = 'Precision flag',
  flag_value = 1, # results that get flagged (could lower threshold?)
  mean_select = 'negative', # Over-dispersed
  flag_null_value = 0, # results that are not flagged for comparison
  variable = 'Precision',
  n_select = 3
)
gplot_over2 = ggplot(data=selected_papers, aes(x=t, fill=factor(type)))+
  scale_fill_manual("Potential\nissue", values=c('tomato2','grey66'), labels=c('Yes','No'))+
  geom_histogram()+
  xlab('t-statistic')+
  ylab('Count')+
  scale_y_continuous(labels = label_number(accuracy=1))+ # no decimal places in 
  facet_wrap(~pmcid_ordered, scales='free_y')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())
gplot_over2
```

The plot above is after excluded precision multipliers that were smaller than `r exclude`.

# b1) Under-dispersed

```{r, fig.width=10, fig.height=7}
## show the t-stats of selected papers
selected_papers = select_papers(
  in_data = for_model,
  stats = all_stats,
  flag = 'Precision flag',
  flag_value = 1, 
  mean_select = 'positive', #  Under-dispersed
  flag_null_value = 0, # 
  variable = 'Precision',
  n_select = 3
)
gplot_under = ggplot(data=selected_papers, aes(x=t, fill=factor(type)))+
  scale_fill_manual("Potential\nissue", values=c('tomato2','grey66'), labels=c('Yes','No'))+
  geom_histogram()+
  xlab('t-statistic')+
  ylab('Count')+
  scale_y_continuous(labels = label_number(accuracy=1))+ # no decimal places in 
  facet_wrap(~pmcid_ordered, scales='free_y')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())
gplot_under
```

# b2) Under-dispersed after excluding large precision multipliers

```{r, fig.width=10, fig.height=7}
exclude = 10 # for excluding large multipliers
## show the t-stats of selected papers
selected_papers = select_papers(
  in_data = for_model,
  stats = all_stats,
  exclude = exclude, # exclude multipliers over this
  flag = 'Precision flag',
  flag_value = 1, 
  mean_select = 'positive', # under-dispersed
  flag_null_value = 0, # 
  variable = 'Precision',
  n_select = 3
)
gplot_under2 = ggplot(data=selected_papers, aes(x=t, fill=factor(type)))+
  scale_fill_manual("Potential\nissue", values=c('tomato2','grey66'), labels=c('Yes','No'))+
  geom_histogram()+
  xlab('t-statistic')+
  ylab('Count')+
  scale_y_continuous(labels = label_number(accuracy=1))+ # no decimal places in 
  facet_wrap(~pmcid_ordered, scales='free_y')+
  theme_bw()+
  theme(panel.grid.minor = element_blank())
gplot_under2
```

We exclude multipliers over `r exclude`.

```{r, include=FALSE}
## save for paper
save(gplot_over, gplot_over2, gplot_under, gplot_under2, file=example_outfile)
```

# P-value distributions

Here we examine the distributions of p-values and t-statistics to show the differences.
We try to take the results of a typical simulation by choosing a simulation that most closely matches the average mean and standard deviation for p-values across all simulations. We restrict the data to trials with 30 or more rows in the table in order to show distributions with a reasonable sample size.

### Distributions of p-values for selected trials

```{r}
# get the table data with p-values
stage = 'model'
source('1_which_data_source.R') # uses `source` and `stage`
#
for_model = make_stats_for_bayes_model(indata = table_data) # see 99_functions.R
# add study type if it is a simulation
if(simulation == FALSE){ # from 1_which_data_source.R
  for_model = mutate(for_model, sgroup='None', h0='N/A')
}
if(simulation == TRUE){ # from 1_which_data_source.R
  for_model = mutate(for_model,
                     h0 = case_when( # group for plotting
str_detect(pmcid, pattern='small') ~ 'No',
str_detect(pmcid, pattern='large') ~ 'No',
str_detect(pmcid, pattern='skewed') ~ 'No',
str_detect(pmcid, pattern='rounded') ~ 'No',
str_detect(pmcid, pattern='none') ~ 'No',
str_detect(pmcid, pattern='precise') ~ 'Yes',
str_detect(pmcid, pattern='variable') ~ 'Yes'
  ),
      sgroup = case_when( # group for plotting
str_detect(pmcid, pattern='small') ~ 'Small binary',
str_detect(pmcid, pattern='large') ~ 'Large binary',
str_detect(pmcid, pattern='skewed') ~ 'Skewed',
str_detect(pmcid, pattern='rounded') ~ 'Rounded',
str_detect(pmcid, pattern='none') ~ 'As expected',
str_detect(pmcid, pattern='precise') ~ 'Under-dispersed',
str_detect(pmcid, pattern='variable') ~ 'Over-dispersed'
  ))
}
# show example distribution for each issue
stats = group_by(for_model, h0, sgroup, pmcid) %>%
  summarise(n = n(),
            mean = mean(p), # get mean per study
            sd = sd(p)) %>%
  filter(!is.na(sd),
         n >= 30) # at least 30 p-values to show
overall = group_by(stats, h0, sgroup) %>%
  summarise(N = n(),
            omean = mean(mean), # get overall mean of means
            osd = mean(sd)) 
# now find closest matching example
close = left_join(stats, overall, by=c('h0','sgroup')) %>%
  mutate(mdiff = mean - omean,
         sdiff = sd - osd,
         distance = sqrt(mdiff^2 + sdiff^2)) %>%
  group_by(h0, sgroup) %>%
  arrange(h0, sgroup, distance) %>%
  slice(1) # just one trial
# now get p-value distributions for close matches
selected = filter(for_model, pmcid %in% close$pmcid) 
pdist = ggplot(data = selected, aes(x=p, fill=h0))+
  geom_histogram()+
  scale_x_continuous(limits=c(-0.05,1.05))+
  scale_fill_manual('Difference between groups', values=c('skyblue','darkorange'))+
  theme_bw()+
  theme(panel.grid.minor = element_blank(),
        legend.position = 'top')+
  facet_wrap(~sgroup)
if(simulation==FALSE){
  pdist = pdist + theme(legend.position = 'none')
}
pdist
```

### Distributions of t-statistics for the same selected trials

```{r}
theme_set(theme_bw())
# 
standard = data.frame(t = rnorm(100000))
#
tdist = gf_dhistogram( ~ t | sgroup, data = selected) %>%
  gf_fitdistr( dist = 'dnorm', color = "blue") %>%
  gf_fitdistr( ~ t, dist = 'dnorm', color = "red", data=standard) +
  theme_bw()+
  theme(panel.grid.minor = element_blank())
tdist
# save for supplement
save(tdist, pdist, file=supplement_outfile)
```

The red line is a standard normal distribution and the blue line is a normal distribution fitted to the observed density. 

### Testing that p-values for a uniform distribution

To summarise the simulations we tested if the p-values for each study followed a uniform distribution using the Kolmogorov--Smirnov test for uniformity. The table below shows the frequency that were statistically significant using a 0.05 threshold. This result is more relevant for the simulation data than the real data.

```{r}
# test uniform distribution for p-values
unif_pvals = group_by(for_model, h0, sgroup, pmcid) %>%
  summarise(p = kolmogorov.unif.test(p, nrepl=2000, k=0)$p.value)
# summary stats
unif_tab = mutate(unif_pvals, sig = p<0.05) %>%
  summarise(below = sum(sig), n=n()) %>%
  mutate(prop = roundz(100*below/n, 1),
         cell = paste(below, ' (', prop, ')', sep='')) %>%
  arrange(h0, sgroup, below) 
renamed = select(unif_tab, h0, sgroup, cell) %>%
  rename('H0' = 'h0',
    'Simulation' = 'sgroup',
    'n (%)' = 'cell')
# nice table  
if(simulation==FALSE){
  ftab = select(renamed, -Simulation, -H0) %>%
    flextable() %>%
    theme_box() %>%
    merge_v(j=1) %>%
    autofit()
  ftab
}
if(simulation==TRUE){
  ftab = flextable(renamed) %>%
    theme_box() %>%
    merge_v(j=1) %>%
    autofit()
  ftab
}
```

### Histogram(s) of p-values for uniform test

```{r}
hplot = ggplot(unif_pvals, aes(x=p, fill=h0))+
  geom_histogram()+
  xlab('P-value')+
  ylab('Count')+
  scale_fill_manual('Difference between groups', values=c('skyblue','darkorange'))+
  theme_bw()+
  theme(panel.grid.minor = element_blank(),
        legend.position = 'none')
if(simulation=='TRUE'){ # add facet wrap and legend if simulation
    hplot = hplot + facet_wrap(~sgroup)+
      theme(legend.position = 'top')
}
hplot
```

```{r, include=FALSE}
## save results for use in paper
# make combined table 
if(simulation == FALSE){
  my_tab = mutate(my_tab, sgroup='None')
}
if(simulation == TRUE){
  my_tab = rename(my_tab, 'sgroup'='simulation')
}
overall_table = left_join(my_tab, unif_tab, by='sgroup')
# save
save(tdist, pdist, overall_table, ftab_results, hplot, file=results_outfile)
```

For simulated results the histograms are split by the simulation type.
