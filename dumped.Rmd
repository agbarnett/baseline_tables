# from abstract

*Objective*: To automatically screen for under- and over-dispersion in the baseline tables of randomised trials submitted to journals. 

*Design*: Observational study and simulation study. 

*Setting*: Randomised controlled trials published in open access health and medical journals on _PubMed Central_. 

*Outcome measures*: The estimated probability that a trial's summary statistics were under- or over-dispersed using a Bayesian model that compared the distribution of t-statistics for the between-group differences from the baseline table to an expected distribution without dispersion.

*Results*: The algorithm had a relatively good accuracy for extracting tables, matching well on the size of the tables and sample size. Using t-statistics in the Bayesian model out-performed the uniform test of p-values, as it had a higher probability of detecting over-dispersed data and lower false positive percentage for skewed data that was not under- or over-dispersed. For the trials published on _PubMed Central_, tables were flagged because they were not from trials, had a non-standard presentation, or had likely reporting errors. The algorithm also flagged some trials as under-dispersed where there was a striking similarity between groups.

*Conclusion*: Due to the variance in reporting across journals, the current method may not be suitable for automated screening for fraud of all submitted trials, but could be useful in targeted checks of suspected trials. If used in automated screening of papers at the submission stage, it would likely flag many instances of non-standard table presentation and poor reporting --- which would still benefit from being fixed.


#

In this comparison we include some tables that would be excluded in the main analysis because they are not RCTs. These tables are still useful here though, as they allow us to test the accuracy of the algorithm on similar tables.


# from 3_compare_algorithm_hand.Rmd

The results are ordered by the reason for rejection from the algorithm.

```{r}
both = full_join(excluded_hand, excluded_algorithm, by='pmcid') %>%
  select(pmcid, hand, algorithm) %>%
  unique() %>%
  arrange(algorithm) # sort by reason
ftab = flextable(both) %>%
  fontsize(size=8, part = 'all') %>%
  autofit()
ftab
```


#### Table as above but just for first three rows of the table

```{r}
stats = filter(matches, row <= 3) %>%
  select(pmcid, statistic.y, starts_with('match')) %>%
  pivot_longer(cols=starts_with('match')) %>%
  filter(!is.na(value)) %>% # remove missing comparisons (both stats needed) %>%
  group_by(statistic.y, name) %>%
  summarise(n = n(), 
            match = sum(value)) %>%
  mutate(percent = 100*match/n,
         stat = case_when(
           name == 'match_stat1' ~ 'First',
           name == 'match_stat2' ~ 'Second'
         ),
         cell = paste(match, ' / ', n, ' (', roundz(percent,1),')', sep='')) %>%
  select(statistic.y, stat, cell) %>%
  rename('Statistic type' = 'statistic.y',
         'Statistic' = 'stat',
         'n / N (%)' = 'cell')
#
ftab = flextable(stats) %>%
  theme_box() %>%
  merge_v(j=1) %>%
  autofit()
ftab

```

#### From 5_summary_stats.Rmd

### Boxplot by table column

```{r}
# 
sizes = dplyr::select(table_data, pmcid, column, sample_size) %>%
  unique()
#
gplot = ggplot(data=sizes, aes(x=factor(column), y=sample_size))+
  geom_boxplot()+
  scale_y_log10()+
  ylab('Sample size')+
  xlab('Table column')+
  theme_bw()
gplot
```

### Parametric fit to log sample size

```{r, include=FALSE}
fg = fitdist(data=log(sizes$sample_size), distr="gamma") # spits stuff to screen
max_size = max(sizes$sample_size)
N = nrow(sizes)
pred = dgamma(x=log(1:max_size), shape=fg$estimate[1], rate=fg$estimate[2])
gamma_line = data.frame(row = log(1:max_size), fitted = pred*N)
# make random data for easy plotting in histogram
rgamma = data.frame(rgamma(n=N, shape=fg$estimate[1], rate=fg$estimate[2]))
names(rgamma) = 'sample_size'
rgamma = mutate(rgamma, sample_size = round(exp(sample_size)))
```

```{r}
# 
to_plot = bind_rows(sizes, rgamma, .id='source') %>%
  mutate(source = ifelse(source==1, 'Observed data', 'Fitted from gamma'))
#
gplot = ggplot(data=to_plot, aes(x=log(sample_size)))+
  geom_histogram(fill='skyblue') +
  ylab('Count')+
  xlab('Log-transformed sample size')+
  theme_bw()+
  facet_wrap(~source)
gplot
```


### from validation 3_compare_algorithm_hand.Rmd

# P-values

This section examines differences in the detection of p-values.

```{r, pvalues}
pvals = full_join(p_hand, p_algorithm, by='pmcid')
#
compare = group_by(pvals, in_table, reported) %>%
  filter(pmcid != 'PMC7549819') %>% # exclude non-RCT
  mutate(in_table = case_when(
    in_table == TRUE ~ 'Yes',
    in_table == FALSE ~ 'No',
    is.na(in_table) == TRUE ~ 'Missing'
  ),
  reported = case_when(
    reported == 'yes' ~ 'Yes',
    reported == 'no' ~ 'No',
    reported == '' ~ 'Missing', 
    is.na(reported) == TRUE ~ 'Missing'
  )) %>%
  rename('Algorithm' = 'in_table',
         'manual' = 'reported') 
# one to check
check_pvals = filter(compare, Algorithm != manual)
# table
tab = tabyl(compare, manual, Algorithm) %>%
  adorn_totals("row") 
#
ftab = flextable(tab) %>%
  add_header_row(values=c(' ', rep('Algorithm', 3))) %>%
  merge_at(i=1, j=2:4, part='header') %>%
  theme_box() %>%
  autofit() %>%
  bg(i=1, j=2, part='body', bg='grey77') %>% # shade diagonals
  bg(i=2, j=3, part='body', bg='grey77') %>% # shade diagonals
  bg(i=3, j=4, part='body', bg='grey77')  # shade diagonals
ftab
# binomial confidence interval for error
x = filter(compare, manual == Algorithm,
           Algorithm !='Missing',
           manual !='Missing')
x = nrow(x) # number of matches;
n_pval = x
pval_bin = binom.confint(x=x, n=x, method='exact') %>%
  mutate(lower = roundz(lower*100,1),
         upper = roundz(upper*100,0))
```

This table includes more results than the other analyses as we included two papers that would be excluded for our algorithm but were useful for this comparison. 

The exact 95% confidence interval for the percentage of matches is `r pval_bin$lower`% to `r pval_bin$upper`%. Hence the algorithm has a high accuracy for delineating if p-values were included in baseline tables. We excluded the results that were missing for the algorithm or manual entry, as these missing were not about the p-value but about the study design and exclusion criteria.
